{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг. Имплементации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В sklearn есть наивная версия версия градиентного бустинга, [придуманная](https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451) в 1999 году Фридманом. С тех пор было предложено много реализаций, которые оказываются лучше на практике. На сегодняшний день популярны три библиотеки, реализующие градиентный бустинг:\n",
    "* XGBoost. После выхода быстро набрала популярность и оставалась стандартом до конца 2016 года.\n",
    "* LightGBM. Отличительной чертой является быстрота построения композиции. Например, используется следующий трюк для ускорения обучения: при построении вершины дерева вместо перебора по всем значениям признака производится перебор значений гистограммы этого признака. Таким образом, вместо $O(\\ell)$ требуется $O$(#bins). Кроме того, в отличие от других библиотек, которые строят дерево по уровням, LightGBM использует стратегию best-first, т.е. на каждом шаге строит вершину, дающую наибольшее уменьшение функционала. Таким образом каждое дерево является цепочкой с прикрепленными листьями, поэтому ограничение на num_leaves получается более осмысленным.\n",
    "* CatBoost. Библиотека от компании Яндекс. Позволяет автоматически обрабатывать категориальные признаки. Кроме того, алгоритм является менее чувствительным к выбору конкретных гиперпараметров. За счёт этого уменьшается время, которое тратит человек на подбор оптимальных гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as gbm\n",
    "import catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"flights.csv\")\n",
    "data = data.sample(frac=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1\n",
    "\n",
    "1) Оставьте колонки \"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\",\n",
    "                 \"ORIGIN_AIRPORT\",\"AIR_TIME\", \"DEPARTURE_TIME\",\"DISTANCE\",\"ARRIVAL_DELAY\"\n",
    "                 \n",
    "2) Удалите пропуски в датасете\n",
    "\n",
    "3) Создайте столбец  target с 1 и 0: 1, если опоздание более чем на 10 минут, 0 - меньше или ровно 10 минут\n",
    "\n",
    "4) Для листа cols закодируйте колонки с категориальными переменными в числа (т.е. текстовым переменным поставим в соответствие числа\n",
    "\n",
    "5) Разбейте данные на тренировочную и тестовую выборку в соотношении 0.75, 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\",\"ORIGIN_AIRPORT\"]\n",
    "\n",
    "\n",
    "'''Your code here.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2\n",
    "\n",
    "Создайте пользовательскую функцию print_auc с тремя параметрами:\n",
    "- model: модель, которая будет делать прогнозы\n",
    "- train: тренировочные данные\n",
    "- test: тестовые данные\n",
    "\n",
    "Функция возвращает два значения: roc_auc для тренировочной выборки и roc_auc для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Your code here.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все бустинговые библиотеки поддерживают формат обучения моделей, как в sklearn, так и свои собственные элементы, поэтому помимо основой части кода с датасетом про полеты мы еще будем смотреть, как работать в логике самой библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cчитывание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так, например, можно задать данные, чтобы считывание проходило быстрее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но данные у нас уже записаны и можем воспользоваться классическим подходом по типу sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, параметры задают отдельно в словаре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\"max_depth\": 10,\n",
    "             \"min_child_weight\" : 2,\n",
    "              \"n_estimators\": 200,\n",
    "              \"learning_rate\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(params_xgb, dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "Но можно и обучить в стиле sklearn: \n",
    "\n",
    "- Объявите модель xgb.XGBClassifier с параметрами выше\n",
    "- Обучите модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Your code here.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогноз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Прогнозы получаются стандартно - методом predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считывание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model('model.bin')  # load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM/ Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные параметры\n",
    "\n",
    "(lightgbm/catboost)\n",
    "\n",
    "* objective – функционал, на который будет настраиваться композиция\n",
    "* eta / learning_rate – темп (скорость) обучения\n",
    "* num_iterations / n_estimators  – число итераций бустинга\n",
    "\n",
    "#### Параметры, отвечающие за сложность деревьев\n",
    "* max_depth – максимальная глубина \n",
    "* max_leaves / num_leaves – максимальное число вершин в дереве\n",
    "* gamma / min_gain_to_split – порог на уменьшение функции ошибки при расщеплении в дереве\n",
    "* min_data_in_leaf – минимальное число объектов в листе\n",
    "* min_sum_hessian_in_leaf – минимальная сумма весов объектов в листе, минимальное число объектов, при котором делается расщепление \n",
    "* lambda – коэффициент регуляризации (L2)\n",
    "* subsample / bagging_fraction – какую часть объектов обучения использовать для построения одного дерева \n",
    "* colsample_bytree / feature_fraction – какую часть признаков использовать для построения одного дерева \n",
    "\n",
    "Подбор всех этих параметров — настоящее искусство. Но начать их настройку можно с самых главных параметров: learning_rate и n_estimators. Обычно один из них фиксируют, а оставшийся из этих двух параметров подбирают (например, фиксируют n_estimators=1000 и подбирают learning_rate). Следующим по важности является max_depth. В силу того, что мы заинтересованы в неглубоких деревьях, обычно его перебирают из диапазона [3; 7].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gbm = {\n",
    "    \"objective\": 'binary',\n",
    "    \"max_depth\": 10,\n",
    "    \"min_child_weight\" : 2,\n",
    "    \"n_estimators\": 200,\n",
    "    \"learning_rate\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(train, label=y_train)\n",
    "\n",
    "# Without Categorical Features\n",
    "model2 = lgb.train(params_gbm, d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Catgeorical Features\n",
    "cats_features_name = [\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"DESTINATION_AIRPORT\",\n",
    "                 \"ORIGIN_AIRPORT\"]\n",
    "model2 = lgb.train(params, d_train, categorical_feature=cats_features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.booster_.save_model('lightgbm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
    "clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_index = [0,1,2,3,4,5,6]\n",
    "\n",
    "# With Categorical features\n",
    "clf = cb.CatBoostClassifier(eval_metric=\"AUC\",one_hot_max_size=31, \\\n",
    "                            depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
    "\n",
    "clf.fit(train,y_train, cat_features=cat_features_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model('catboost.cbm', format='cbm')\n",
    "clf = clf.load_model('catboost.cbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Образец гридсерча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = lgb.LGBMClassifier(silent=False)\n",
    "param_dist = {\"max_depth\": [25,50, 75],\n",
    "              \"learning_rate\" : [0.01,0.05,0.1],\n",
    "              \"num_leaves\": [300,900,1200],\n",
    "              \"n_estimators\": [200]\n",
    "             }\n",
    "grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "grid_search.fit(train, y_train)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM боевого образца\n",
    "(пишем вместе)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Your code here.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стэкинг моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    error = (y_true - y_pred) ** 2\n",
    "    return np.sqrt(np.mean(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "\n",
    "- Напишите функцию rmse, рассчитывающую rmse для прогнозных и фактических значениях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Your code here.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5\n",
    "\n",
    "- Обучите CatBoostRegressor\n",
    "- Сохраните прогнозы для тестовой выборки в переменной y_pred_cbm\n",
    "- Сохраните прогнозы для тренировочной выборки в переменной y_train_pred_cbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cbm = CatBoostRegressor(iterations=100, max_depth=4, learning_rate=0.01, loss_function='RMSE', logging_level='Silent')\n",
    "\n",
    "\n",
    "'''Your code here.'''\n",
    "\n",
    "\n",
    "print(\"Train RMSE GB = %.4f\" % rmse(y_train, y_train_pred_cbm))\n",
    "print(\"Test RMSE GB = %.4f\" % rmse(y_test, y_pred_cbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6\n",
    "\n",
    "- Отшкалируйте данные\n",
    "- Обучите линейную регрессию\n",
    "- Сохраните прогнозы для тестовой выборки в переменной y_pred_lr\n",
    "- Сохраните прогнозы для тренировочной выборки в переменной y_train_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "'''Your code here.\n",
    "'''\n",
    "\n",
    "print(\"Train RMSE LR = %.4f\" % rmse(y_train, y_train_pred_lr))\n",
    "print(\"Test RMSE LR = %.4f\" % rmse(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты будем считать, что новый алгоритм $a(x)$ представим как\n",
    "$$\n",
    "    a(x)\n",
    "    =\n",
    "    \\sum_{n = 1}^{N}\n",
    "    w_n b_n(x),\n",
    "$$\n",
    "где $\\sum_{n} w_n =1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_weights(y_true, y_pred_1, y_pred_2):\n",
    "    grid = np.linspace(0, 1, 1000)\n",
    "    metric = []\n",
    "    for w_0 in grid:\n",
    "        w_1 = 1 - w_0\n",
    "        y_a = w_0 * y_pred_1 + w_1 * y_pred_2\n",
    "        metric.append([rmse(y_true, y_a), w_0, w_1])\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_blending_train, w_0, w_1 = min(select_weights(y_train, y_train_pred_cbm, y_train_pred_lr), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_blending_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_pred_cbm * w_0 +  y_pred_lr * w_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге получаем качество на тестовой выборке лучше, чем у каждого алгоритма в отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезные ссылки:\n",
    "\n",
    "\n",
    "* [Документация XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "* [Документация LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)\n",
    "* [Документация CatBoost](https://catboost.ai/docs/concepts/python-quickstart.html)\n",
    "* [Видео про стекинг](https://www.coursera.org/lecture/competitive-data-science/stacking-Qdtt6)\n",
    "* [Продвинутый вариант стекинга с использованием нейросетей](https://www.coursera.org/learn/competitive-data-science/lecture/s8RLi/stacknet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
