{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков и понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом семинаре мы рассмотрели полный путь решения задачи от визуализации и предобработки до стекинга и блендинга. Пройдем его ещё раз сосредоточившись на визуализации и отборе признаков.\n",
    "\n",
    "Понижение размерности (визуализация в 2D это частный случай понижения размерности до 2-х измерений) можно использовать и для других целей:\n",
    "\n",
    "* Сокращение ресурсоемкости алгоритмов\n",
    "* Ослабление влияния проклятия размерности и тем самым уменьшение переобучения\n",
    "* Переход к более информативным признакам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "Самый простой способ выделения признаков &mdash; их отбор. Не будем заострять много внимания\n",
    "на этом методе, так как он очень простой, просто приведем пример, показывающий, что\n",
    "так можно примитивно сокращать ресурсоемкость алгоритмов.\n",
    "\n",
    "Отберем признаки на основе их корреляции с целевым признаком, и сравним результаты с исходными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "ds = load_boston()\n",
    "X, y = ds.data, ds.target\n",
    "indices = np.arange(len(y))\n",
    "np.random.seed(52342)\n",
    "np.random.shuffle(indexes)\n",
    "X = X[indices, :]\n",
    "y = y[indices]\n",
    "\n",
    "features_ind = np.arange(X.shape[1])\n",
    "corrs = np.abs([pearsonr(X[:, i], y)[0] for i in features_ind])\n",
    "importances_sort = np.argsort(corrs)\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.barh(features_ind, corrs[importances_sort])\n",
    "plt.xlabel('importance', fontsize=20)\n",
    "X = X[:, importances_sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "features_counts = np.arange(X.shape[1])\n",
    "\n",
    "def scores_by_features_count(reg):\n",
    "    scores = []\n",
    "    for features_part in features_counts:\n",
    "        X_part = X[:,importances_sort[features_part:]]\n",
    "        scores.append(cross_val_score(reg, X_part, y).mean())\n",
    "    return scores\n",
    "\n",
    "linreg_scores = scores_by_features_count(LinearRegression())\n",
    "rf_scores = scores_by_features_count(RandomForestRegressor(n_estimators=100, max_depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.plot(features_counts, linreg_scores, label='LinearRegression')\n",
    "plt.plot(features_counts, rf_scores, label='RandomForest')\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.xlabel('#features deleted', fontsize=20)\n",
    "plt.ylabel('$R^2$', fontsize=20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем, если мы захотим немного сократить потребление ресурсов, пожертвовав частью качества,\n",
    "видно, что это можно сделать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (Principal Component Analysis, PCA)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и\n",
    "в некоторых ситуациях такой подход практически бесполезен. Например, если\n",
    "мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о\n",
    "содержимом картинки. \n",
    "\n",
    "Поэтому признаки нужно как-то комбинировать. Рассмотрим метод главных компонент.\n",
    "\n",
    "Этот метод делает два важных упрощения задачи\n",
    "\n",
    "1. Игнорируется целевая переменная\n",
    "2. Строится линейная комбинация признаков\n",
    "\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является\n",
    "таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то\n",
    "внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с\n",
    "целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA на плоскости\n",
    "\n",
    "Для начала посмотрим на метод PCA на плоскости для того, чтобы\n",
    "лучше понять, как он устроен.\n",
    "\n",
    "Попробуем специально сделать один из признаков более значимым и проверим, что PCA это обнаружит. Сгенерируем выборку из двухмерного гауссовского распределения. Обратите внимание, что выборка\n",
    "изначально выбирается центрированной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314512)\n",
    "\n",
    "data_synth_1 = np.random.multivariate_normal(\n",
    "    mean=[0, 0], \n",
    "    cov=[[4, 0], \n",
    "         [0, 1]],\n",
    "    size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент.\n",
    "В результате работы PCA из sklearn в `dec.components_` будут лежать главные направления (нормированные), а в `dec.explained_variance_` &mdash; дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их\n",
    "значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def PCA_show(dataset):\n",
    "    plt.scatter(*zip(*dataset), alpha=0.5)\n",
    "    \n",
    "    dec = PCA()\n",
    "    dec.fit(dataset)\n",
    "    ax = plt.gca()\n",
    "    for comp_ind in range(dec.components_.shape[0]):\n",
    "        component = dec.components_[comp_ind, :]\n",
    "        var = dec.explained_variance_[comp_ind]\n",
    "        start, end = dec.mean_, component * var\n",
    "        ax.arrow(start[0], start[1], end[0], end[1],\n",
    "                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n",
    "    \n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав\n",
    "дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим,\n",
    "что для PCA это ничего не изменит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 6\n",
    "rotate = np.array([\n",
    "        [np.cos(angle), - np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "    ])\n",
    "data_synth_2 = rotate.dot(data_synth_1.T).T\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну вот, все нормально. \n",
    "\n",
    "Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
    "\n",
    "np.random.seed(54242)\n",
    "data_synth_bad = [\n",
    "    make_circles(n_samples=1000, factor=0.2, noise=0.1)[0]*2,\n",
    "    make_moons(n_samples=1000, noise=0.1)[0]*2,\n",
    "    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n",
    "    np.random.multivariate_normal(\n",
    "        mean=[0, 1.5], \n",
    "        cov=[[3, 1], \n",
    "             [1, 1]],\n",
    "        size=1000),\n",
    "]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "rows, cols = 2, 2\n",
    "for i, data in enumerate(data_synth_bad):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    PCA_show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лица людей\n",
    "\n",
    "Рассмотрим датасет с фотографиями лиц людей и применим к его признакам PCA.\n",
    "\n",
    "Ниже изображены примеры лиц из базы, и последняя картинка &mdash; это \"среднее лицо\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "faces = fetch_olivetti_faces(shuffle=True, random_state=432542)\n",
    "faces_images = faces.data\n",
    "faces_ids = faces.target\n",
    "image_shape = (64, 64)\n",
    "    \n",
    "mean_face = faces_images.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples - 1):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(faces_images[i, :].reshape(image_shape), interpolation='none',\n",
    "               cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "plt.subplot(rows, cols, n_samples)\n",
    "plt.imshow(mean_face.reshape(image_shape), interpolation='none',\n",
    "           cmap='gray')\n",
    "plt.xticks(())\n",
    "_ = plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем главные компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = PCA()\n",
    "faces_images -= mean_face\n",
    "red.fit(faces_images)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "rows, cols = 2, 4\n",
    "n_samples = rows * cols\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    plt.imshow(red.components_[i, :].reshape(image_shape), interpolation='none',\n",
    "               cmap='gray')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось жутковато, что уже неплохо, но есть ли от этого какая-то польза?\n",
    "\n",
    "Во-первых, новые признаки дают более высокое качество классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gscv_rf = GridSearchCV(RandomForestClassifier(),\n",
    "                       {'n_estimators': [100, 200, 500, 800], 'max_depth': [2, 3, 4, 5]},\n",
    "                       cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gscv_rf.fit(faces_images, faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gscv_rf.fit(red.transform(faces_images)[:,:100], faces_ids)\n",
    "print(gscv_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, их можно использовать для компактного хранения данных. Для этого объекты трансформируются\n",
    "в новое пространство, и из него выкидываются самые незначимые признаки.\n",
    "\n",
    "Ниже приведены результаты сжатия в 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_size = image_shape[0] * image_shape[1]\n",
    "\n",
    "def compress_and_show(compress_ratio):\n",
    "    red = PCA(n_components=int(base_size * compress_ratio))\n",
    "    red.fit(faces_images)\n",
    "\n",
    "    faces_compressed = red.transform(faces_images)\n",
    "    faces_restored = red.inverse_transform(faces_compressed) + mean_face\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    rows, cols = 2, 4\n",
    "    n_samples = rows * cols\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(faces_restored[i, :].reshape(image_shape), interpolation='none',\n",
    "                   cmap='gray')\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "compress_and_show(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И даже при сжатии в 20 раз лица остаются узнаваемыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_and_show(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA с ядрами\n",
    "\n",
    "Так как PCA фактически работает не исходными признаками, а с матрицей их ковариаций, можно\n",
    "использовать для ее вычисления вместо скалярного произведения $\\langle x_i, x_j \\rangle$ произвольное\n",
    "ядро $K(x_i, x_j)$. Это будет соответствовать переходу в другое пространство, в котором\n",
    "наше предположение о линейности уже будет иметь смысл. Единственная проблема &mdash; непонятно, как\n",
    "подбирать ядро.\n",
    "\n",
    "Ниже приведены примеры объектов в исходном пространстве (похожие группы обозначены одним цветом\n",
    "для наглядности), и результат их трансформации в новые пространства (для разных ядер). Если результаты\n",
    "получаются линейно разделимыми &mdash; значит мы выбрали подходящее ядро."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "\n",
    "def KPCA_show(X, y):\n",
    "    reds = y == 0\n",
    "    blues = y == 1\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    rows, cols = 2, 2\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.scatter(X[reds, 0], X[reds, 1], alpha=0.5, c='r')\n",
    "    plt.scatter(X[blues, 0], X[blues, 1], alpha=0.5, c='b')\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    kernels_params = [\n",
    "        dict(kernel='rbf', gamma=10),\n",
    "        dict(kernel='poly', gamma=10),\n",
    "        dict(kernel='cosine', gamma=10),\n",
    "    ]\n",
    "    \n",
    "    for i, p in enumerate(kernels_params):\n",
    "        dec = KernelPCA(**p)\n",
    "        X_transformed = dec.fit_transform(X)\n",
    "        \n",
    "        plt.subplot(rows, cols, i + 2)\n",
    "        plt.scatter(X_transformed[reds, 0], X_transformed[reds, 1], alpha=0.5, c='r')\n",
    "        plt.scatter(X_transformed[blues, 0], X_transformed[blues, 1], alpha=0.5, c='b')\n",
    "        ax = plt.gca()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "np.random.seed(54242)\n",
    "KPCA_show(*make_circles(n_samples=1000, factor=0.2, noise=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(54242)\n",
    "KPCA_show(*make_moons(n_samples=1000, noise=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачка самостоятельная\n",
    "\n",
    "На одном из прошлых занятий мы работали с датасетом по текстам. При 1000 текстов мы получили около 1500 признаков. В тот момент мы закрыли глаза на это обстоятельство, но теперь - не будем. \n",
    "\n",
    "Ваша задача -- снизить размерность пространства данных. Попробуем отобрать 100 лучших признаков и на них обучить модель и посмотреть на ее качество:\n",
    "\n",
    "- пробуем t-score\n",
    "- пробуем PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Restaurant_Reviews.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, data['Review'].shape[0]):\n",
    "    ps = PorterStemmer()\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
