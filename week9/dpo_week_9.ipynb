{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг\n",
    "\n",
    "Сравним, как ведут себя бустинг и бэггинг с ростом числа базовых алгоритмов.\n",
    "\n",
    "В случае бэггинга все базовые алгоритмы настраиваются на различные выборки из одного и того же распределения на $\\mathbb{X} \\times \\mathbb{Y}$. При этом некоторые из них могут оказаться переобученными, однако усреднение позволяет ослабить этот эффект (объясняется тем, что для некоррелированных алгоритмов разброс композиции оказывается в $N$ раз меньше разброса отдельных алгоритмов, т.е. много деревьев с меньшей вероятностью настроятся на некоторый нетипичный объект по сравнению с одним деревом). Если $N$ достаточно велико, то последующие добавления новых алгоритмов уже не позволят улучшить качество модели.\n",
    "\n",
    "В случае же бустинга каждый алгоритм настраивается на ошибки всех предыдущих, это позволяет на каждом шаге настраиваться на исходное распределение все точнее и точнее. Однако при достаточно большом $N$ это является источником переобучения, поскольку последующие добавления новых алгоритмов будут продолжать настраиваться на обучающую выборку, уменьшая ошибку на ней, при этом уменьшая обобщающую способность итоговой композиции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.\n",
    "Дана выборка с пятью объектами и двумя признаками, задача регрессии:\n",
    "\n",
    "| Признак 1 | Признак 2 | Ответ |\n",
    "|-----------|-----------|-------|\n",
    "| 1         | -1        | 0.3   |\n",
    "| 2         | 1         | -0.4  |\n",
    "| 3         | 0         | 0.1   |\n",
    "| 1         | 0         | 0.2   |\n",
    "| 2         | -1        | -0.8  |\n",
    "\n",
    "Мы обучаем градиентный бустинг с квадратичной функцией потерь $L(y, z) = (y-z)^2$. Первый базовый алгоритм выбираем очень простым и константным: $b_0(x) = 0$. На какие целевые переменные будет настраиваться второй базовый алгоритм $b_1(x)$?\n",
    "\n",
    "\n",
    "### Задача 2.\n",
    "Рассмотрми ту же выборку, что в предыдущей задаче.\n",
    "Мы обучаем градиентный бустинг с функцией потерь $L(y, z) = |y-z|$. Пусть мы уже построили несколько базовых алгоритмов и нашли их веса, получив композицию $b_0(x)+\\gamma_1 b_1(x)+\\dots + \\gamma_3 b_3(x)$. Пусть предсказания этой композиции на объектах обучающей выборки равны (0.2, -0.5, 0.4, 0, -0.3). На какие целевые переменные будет настраиваться пятый базовый алгоритм $b_4(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.linspace(0, 1, 100)\n",
    "X_test = np.linspace(0, 1, 1000)\n",
    "\n",
    "@np.vectorize\n",
    "def target(x):\n",
    "    return x > 0.5\n",
    "\n",
    "Y_train = target(X_train) + np.random.randn(*X_train.shape) * 0.1\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.scatter(X_train, Y_train, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишите функцию, которая берет на вход reg - объекта класса композиции (в качестве reg будет подаваться случайный лес или градиентный бустинг). Функция должна построить несколько графиков, каждый график визуализирует предсказания композиции из определенного числа деревьев. Следуйте комментариям в функции.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(reg):\n",
    "    plt.figure(figsize=(20, 30))\n",
    "    sizes = [1, 2, 5, 20, 100, 500, 1000, 2000] # варианты числа деревьев\n",
    "    for i, s in enumerate(sizes):\n",
    "        reg.n_estimators = s # устанавливаем число деревьев\n",
    "        plt.subplot(4, 2, i+1) # чтобы сделать много графиков\n",
    "        plt.title('{} trees'.format(s)) # отмечаем число деревьев в названии графика\n",
    "        \n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        # обучите reg по синим точам\n",
    "        \n",
    "        # сделайте предсказания для всего отрезка [0, 1], он хранится в X_test\n",
    "        \n",
    "        # нарисуйте синие точки с помощью scatter\n",
    "        \n",
    "        # постройте график предсказаний с помощью plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(warm_start=True)\n",
    "train_and_plot(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что с некоторого момента итоговая функция перестает меняться с ростом количества деревьев.\n",
    "\n",
    "Теперь проделаем то же самое для градинентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1, warm_start=True)\n",
    "train_and_plot(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг довольно быстро построил истинную зависимость, после чего начал настраиваться уже на конкретные объекты обучающей выборки, из-за чего сильно переобучился.\n",
    "\n",
    "\n",
    "Бороться с этой проблемой можно с помощью выбора очень простого базового алгоритма или\n",
    "же искусственным снижением веса новых алгоритмов при помощи шага $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=0}^N \\eta \\gamma_N b_n(x).$$\n",
    "\n",
    "Такая поправка замедляет обучение по сравнению с бэггингом, но зато позволяет получить менее переобученный алгоритм. Тем не менее, важно понимать, что переобучение всё равно будет иметь место при обучении сколь угодно большого количества базовых алгоритмов для фиксированного $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1, warm_start=True)\n",
    "train_and_plot(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим описанный выше эффект на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.5, test_size=0.5)\n",
    "\n",
    "MAX_ESTIMATORS = 300\n",
    "\n",
    "gbclf = BaggingRegressor(warm_start=True)\n",
    "err_train_bag = []\n",
    "err_test_bag = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_bag.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_bag.append(1 - gbclf.score(X_test, Y_test))\n",
    "    \n",
    "gbclf = GradientBoostingRegressor(warm_start=True, max_depth=2, learning_rate=0.1)\n",
    "err_train_gb = []\n",
    "err_test_gb = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_gb.append(1 - gbclf.score(X_train, Y_train))\n",
    "    err_test_gb.append(1 - gbclf.score(X_test, Y_test))\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_gb, label='GB')\n",
    "plt.plot(err_train_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.plot(err_test_bag, label='Bagging')\n",
    "plt.legend()\n",
    "plt.title('Test')\n",
    "plt.gcf().set_size_inches(15,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг и случайные леса\n",
    "\n",
    "Сравним поведение двух методов построения композиции алгоритмов над деревьями на примере задачи [Kaggle: Predicting a Biological Response](https://www.kaggle.com/c/bioresponse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "data = pd.read_csv('bioresponse.csv')\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "\n",
    "    gbm = GradientBoostingClassifier(n_estimators=150, learning_rate=learning_rate, random_state=241).fit(X_train, y_train)\n",
    "    \n",
    "    l = roc_auc_score\n",
    "\n",
    "    test_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_test)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        test_deviance[i] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    train_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_train)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        train_deviance[i] = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_deviance, 'r', linewidth=2)\n",
    "    plt.plot(train_deviance, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    \n",
    "    plt.title('GBM eta=%.1f, test roc-auc=%.3f, best_est=%d' % (learning_rate, test_deviance.max(), test_deviance.argmax()+1))\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, лучшая композиция построена при $\\eta = 0.1$, включает 24 базовых алгоритма и достигает значения 0.816 на контрольной выборке. При этом случайный лес с таким же количеством базовых алгоритмов уступает градиентному бустингу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=24, random_state=0).fit(X_train, y_train)\n",
    "print ('Train RF ROC-AUC =', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print ('Test RF ROC-AUC = ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим также, что при всём этом случайный лес, в отличие от градиентного бустинга, использует глубокие деревья, требующие вычислительных мощностей для их обучения.\n",
    "\n",
    "Для достижения такого же качества случайному лесу требуется гораздо большее число базовых алгоритмов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(10, 101, 10):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=4).fit(X_train, y_train)\n",
    "    print (n_estimators, 'trees: train ROC-AUC =',  roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]), 'test ROC-AUC =',  roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоследок можно посмотреть [визуализацию](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) градиентного бустинга для решающих деревьев различной глубины для функций различного вида."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
